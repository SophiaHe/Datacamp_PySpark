{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data with PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq3VVHE8UavrBiaaDcqHMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiaHe/Datacamp_PySpark/blob/master/Big_Data_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJJ1YY4SMWvK",
        "colab_type": "text"
      },
      "source": [
        "**Course 1: Introduction to PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JhIs-ITFKUm",
        "colab_type": "text"
      },
      "source": [
        "See what tables are in your cluster by calling spark.catalog.listTables()\n",
        "\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "Get the first 10 rows of flights: \n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "Show the results:\n",
        "flights10.show()\n",
        "\n",
        "Convert the results to a pandas DataFrame:\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "# Create pd_temp\n",
        "pd_temp = pd.DataFrame(np.random.random(10))\n",
        "\n",
        "# Create spark_temp from pd_temp\n",
        "spark_temp = spark.createDataFrame(pd_temp)\n",
        "\n",
        "# Add spark_temp to the catalog\n",
        "spark_temp.createOrReplaceTempView(\"temp\")\n",
        "\n",
        "# Examine the tables in the catalog again\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# Don't change this file path\n",
        "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
        "\n",
        "# Read in the airports data\n",
        "airports = spark.read.csv(file_path, header = True)\n",
        "\n",
        "# Create the DataFrame flights\n",
        "flights = spark.table(\"flights\")\n",
        "\n",
        "# Show the head\n",
        "flights.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH55OT6fyiJO",
        "colab_type": "text"
      },
      "source": [
        "**Course 2: Manipulating Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atfjHIaYytIE",
        "colab_type": "text"
      },
      "source": [
        "# Add duration_hrs\n",
        "flights = flights.withColumn('duration_hrs',flights.air_time/60)\n",
        "\n",
        "# Filter flights by passing a string\n",
        "long_flights1 = flights.filter(\"distance > 1000\")\n",
        "\n",
        "# Filter flights by passing a column of boolean values\n",
        "long_flights2 = flights.filter(flights.distance > 1000)\n",
        "\n",
        "# Select the first set of columns\n",
        "selected1 = flights.select(\"tailnum\",\"origin\",\"dest\")\n",
        "\n",
        "# Select the second set of columns\n",
        "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
        "\n",
        "# Define avg_speed\n",
        "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
        "\n",
        "# Create the same table using a SQL expression\n",
        "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
        "\n",
        "# Find the shortest flight from PDX in terms of distance\n",
        "flights.filter(flights.origin == 'PDX').groupBy().min(\"distance\").show()\n",
        "\n",
        "# Average duration of Delta flights\n",
        "flights.filter(flights.carrier == \"DL\").filter(flights.origin == 'SEA').groupBy().avg(\"air_time\").show()\n",
        "\n",
        "# Total hours in the air\n",
        "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n",
        "\n",
        "# Import pyspark.sql.functions as F\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Standard deviation of departure delay\n",
        "by_month_dest.agg(F.stddev('dep_delay')).show()\n",
        "\n",
        "# Rename the faa column\n",
        "airports = airports.withColumnRenamed('faa','dest')\n",
        "\n",
        "# Join the DataFrames\n",
        "flights_with_airports = flights.join(airports,on = 'dest', how = 'leftouter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ERfB7HKy3Q5",
        "colab_type": "text"
      },
      "source": [
        "**Course 3: Machine Learning Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drLqqHr-y3qR",
        "colab_type": "text"
      },
      "source": [
        "At the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
        "\n",
        "Transformer classes have a .transform() method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class Bucketizer to create discrete bins from a continuous feature or the class PCA to reduce the dimensionality of your dataset using principal component analysis.\n",
        "\n",
        "Estimator classes all implement a .fit() method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a StringIndexerModel for including categorical data saved as strings in your models, or a RandomForestModel that uses the random forest algorithm for classification or regression.\n",
        "\n",
        "#### Spark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals\n",
        "\n",
        "It's important to note that .cast() works on columns, while .withColumn() works on DataFrames. The only argument you need to pass to .cast() is the kind of value you want to create, in string form. For example, to create integers, you'll pass the argument \"integer\" and for decimal numbers you'll use \"double\".\n",
        "\n",
        "dataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\")) \\\\\n",
        "model_data = model_data.withColumn(\"month\", model_data.month.cast('integer'))\n",
        "\n",
        "Convert to an integer \\\\\n",
        "model_data = model_data.withColumn(\"label\", model_data.is_late.cast('integer'))\n",
        "\n",
        "Create a StringIndexer \\\\\n",
        "carr_indexer = StringIndexer(inputCol= 'carrier',outputCol='carrier_index')\n",
        "\n",
        "Create a OneHotEncoder \\\\\n",
        "carr_encoder = OneHotEncoder(inputCol='carrier_index',outputCol='carrier_fact')\n",
        "\n",
        "Make a VectorAssembler \\\\\n",
        "vec_assembler = VectorAssembler(inputCols=['month','air_time','carrier_fact','dest_fact','plane_age'], outputCol='features')\n",
        "\n",
        "Import Pipeline \\\\\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "Make the pipeline \\\\\n",
        "flights_pipe = Pipeline(stages=[dest_indexer,dest_encoder,carr_indexer,carr_encoder,vec_assembler])\n",
        "\n",
        "Fit and transform the data \\\\\n",
        "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
        "\n",
        "Split the data into training and test sets \\\\\n",
        "training, test = piped_data.randomSplit([0.6,0.4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NgxrCHRNswJ",
        "colab_type": "text"
      },
      "source": [
        "**Course 4: Model tuning and selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKaQluTkNz9B",
        "colab_type": "text"
      },
      "source": [
        "Import LogisticRegression \\\\\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "Create a LogisticRegression Estimator \\\\\n",
        "lr = LogisticRegression()\n",
        "\n",
        "Import the evaluation submodule \\\\\n",
        "import pyspark.ml.evaluation as evals\n",
        "\n",
        "Create a BinaryClassificationEvaluator \\\\\n",
        "evaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
        "\n",
        "Import the tuning submodule \\\\\n",
        "import pyspark.ml.tuning as tune\n",
        "\n",
        "Create the parameter grid \\\\\n",
        "grid = tune.ParamGridBuilder()\n",
        "\n",
        "Add the hyperparameter \\\\\n",
        "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
        "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
        "\n",
        "Build the grid \\\\\n",
        "grid = grid.build()\n",
        "\n",
        "Create the CrossValidator \\\\\n",
        "cv = tune.CrossValidator(estimator=lr,\n",
        "               estimatorParamMaps=grid,\n",
        "               evaluator=evaluator\n",
        "               )\n",
        "\n",
        "Use the model to predict the test set \\\\\n",
        "test_results = best_lr.transform(test)\n",
        "\n",
        "Evaluate the predictions \\\\\n",
        "print(evaluator.evaluate(test_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1htp678qTXO0",
        "colab_type": "text"
      },
      "source": [
        "###**Big data foundation with PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfhM27ueTJTd",
        "colab_type": "text"
      },
      "source": [
        "**Course 1: Introduction to big data analysis in Spark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guiOE0fFThzC",
        "colab_type": "text"
      },
      "source": [
        "Print the version of SparkContext \\\\\n",
        "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
        "\n",
        "Print the Python version of SparkContext \\\\\n",
        "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
        "\n",
        "Print the master of SparkContext \\\\\n",
        "print(\"The master of Spark Context in the PySpark shell is\", sc.master)\n",
        "\n",
        "Create a python list of numbers from 1 to 100 \\\\\n",
        "numb = range(1, 100)\n",
        "\n",
        "Load the list into PySpark  \\\\\n",
        "spark_data = sc.parallelize(numb)\n",
        "\n",
        "Load a local file into PySpark shell \\\\\n",
        "lines = sc.textFile(file_path)\n",
        "\n",
        "**Use of lambda() with map():** \\\\\n",
        "The map() function in Python returns a list of the results after applying the given function to each item of a given iterable (list, tuple etc.). The general syntax of map() function is map(fun, iter). We can also use lambda functions with map(). The general syntax of map() function with lambda() is map(lambda <agument>:<expression>, iter)\n",
        "\n",
        "Square all numbers in my_list \\\\\n",
        "squared_list_lambda = list(map(lambda x: x ** 2, my_list))\n",
        "\n",
        "Print the result of the map function \\\\\n",
        "print(\"The squared numbers are\", squared_list_lambda)\n",
        "\n",
        "**Use of lambda() with filter():** \\\\\n",
        "Another function that is used extensively in Python is the filter() function. The filter() function in Python takes in a function and a list as arguments. The general syntax of the filter() function is filter(function, list_of_input). Similar to the map(), filter() can be used with lambda() function. The general syntax of the filter() function with lambda() is filter(lambda <argument>:<expression>, list)\n",
        "\n",
        "Filter numbers divisible by 10 \\\\\n",
        "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0sVpyzuY-sx",
        "colab_type": "text"
      },
      "source": [
        "**Course 2: Programming in PySpark RDD’s**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz8xlWOwZBTM",
        "colab_type": "text"
      },
      "source": [
        "Create an RDD from a list of words \\\\\n",
        "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
        "\n",
        "Print out the type of the created object \\\\\n",
        "print(\"The type of RDD is\", type(RDD))\n",
        "\n",
        "**Create a fileRDD from file_path \\\\\n",
        "fileRDD = sc.textFile(file_path)**\n",
        "\n",
        "Create a fileRDD_part from file_path with 5 partitions \\\\\n",
        "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
        "\n",
        "Check the number of partitions in fileRDD_part \\\\\n",
        "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())\n",
        "\n",
        "**Create map() transformation to cube numbers \\\\\n",
        "cubedRDD = numbRDD.map(lambda x: x ** 3)**\n",
        "\n",
        "Collect the results \\\\\n",
        "numbers_all = cubedRDD.collect()\n",
        "\n",
        "Print the numbers from numbers_all \\\\\n",
        "for numb in numbers_all:\n",
        "\tprint(numb)\n",
        "\n",
        "**Filter the fileRDD to select lines with Spark keyword \\\\\n",
        "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)**\n",
        "\n",
        "**How many lines are there in fileRDD? \\\\\n",
        "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())**\n",
        "\n",
        "Print the first four lines of fileRDD \\\\\n",
        "for line in fileRDD_filter.take(4): \n",
        "  print(line)\n",
        "\n",
        "Create PairRDD Rdd with key value pairs \\\\\n",
        "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
        "\n",
        "**Apply reduceByKey() operation on Rdd \\\\\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)**\n",
        "\n",
        "Iterate over the result and print the output \\\\\n",
        "for num in Rdd_Reduced.collect(): \n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
        "\n",
        ":Key 4 has 5 Counts\n",
        "Key 1 has 2 Counts\n",
        "Key 3 has 10 Counts\n",
        "\n",
        "**Sort the reduced RDD with the key by descending order \\\\\n",
        "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)**\n",
        "\n",
        "Iterate over the result and print the output \\\\\n",
        "for num in Rdd_Reduced_Sort.collect():\n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
        "\n",
        ": Key 4 has 5 Counts\n",
        "Key 3 has 10 Counts\n",
        "Key 1 has 2 Counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDQQ-Le7Kr7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following are from https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "# Install spark-related dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azoG2j7XNc8z",
        "colab_type": "code",
        "outputId": "ddc07371-58c6-4c1e-f913-142195b1605d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5ZK1iuOUy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download datasets directly to your Google Drive \"Colab Datasets\" folder\n",
        "\n",
        "import requests\n",
        "\n",
        "# 2007 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)\n",
        "\n",
        "# 2008 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}