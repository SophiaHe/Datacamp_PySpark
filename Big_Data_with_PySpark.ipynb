{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data with PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBup+2C1GBsFxqIigUYxow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiaHe/Datacamp_PySpark/blob/master/Big_Data_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJJ1YY4SMWvK",
        "colab_type": "text"
      },
      "source": [
        "**Course 1: Introduction to PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JhIs-ITFKUm",
        "colab_type": "text"
      },
      "source": [
        "See what tables are in your cluster by calling spark.catalog.listTables()\n",
        "\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "Get the first 10 rows of flights: \n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "Show the results:\n",
        "flights10.show()\n",
        "\n",
        "Convert the results to a pandas DataFrame:\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "# Create pd_temp\n",
        "pd_temp = pd.DataFrame(np.random.random(10))\n",
        "\n",
        "# Create spark_temp from pd_temp\n",
        "spark_temp = spark.createDataFrame(pd_temp)\n",
        "\n",
        "# Add spark_temp to the catalog\n",
        "spark_temp.createOrReplaceTempView(\"temp\")\n",
        "\n",
        "# Examine the tables in the catalog again\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# Don't change this file path\n",
        "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
        "\n",
        "# Read in the airports data\n",
        "airports = spark.read.csv(file_path, header = True)\n",
        "\n",
        "# Create the DataFrame flights\n",
        "flights = spark.table(\"flights\")\n",
        "\n",
        "# Show the head\n",
        "flights.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH55OT6fyiJO",
        "colab_type": "text"
      },
      "source": [
        "**Course 2: Manipulating Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atfjHIaYytIE",
        "colab_type": "text"
      },
      "source": [
        "# Add duration_hrs\n",
        "flights = flights.withColumn('duration_hrs',flights.air_time/60)\n",
        "\n",
        "# Filter flights by passing a string\n",
        "long_flights1 = flights.filter(\"distance > 1000\")\n",
        "\n",
        "# Filter flights by passing a column of boolean values\n",
        "long_flights2 = flights.filter(flights.distance > 1000)\n",
        "\n",
        "# Select the first set of columns\n",
        "selected1 = flights.select(\"tailnum\",\"origin\",\"dest\")\n",
        "\n",
        "# Select the second set of columns\n",
        "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
        "\n",
        "# Define avg_speed\n",
        "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
        "\n",
        "# Create the same table using a SQL expression\n",
        "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
        "\n",
        "# Find the shortest flight from PDX in terms of distance\n",
        "flights.filter(flights.origin == 'PDX').groupBy().min(\"distance\").show()\n",
        "\n",
        "# Average duration of Delta flights\n",
        "flights.filter(flights.carrier == \"DL\").filter(flights.origin == 'SEA').groupBy().avg(\"air_time\").show()\n",
        "\n",
        "# Total hours in the air\n",
        "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n",
        "\n",
        "# Import pyspark.sql.functions as F\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Standard deviation of departure delay\n",
        "by_month_dest.agg(F.stddev('dep_delay')).show()\n",
        "\n",
        "# Rename the faa column\n",
        "airports = airports.withColumnRenamed('faa','dest')\n",
        "\n",
        "# Join the DataFrames\n",
        "flights_with_airports = flights.join(airports,on = 'dest', how = 'leftouter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ERfB7HKy3Q5",
        "colab_type": "text"
      },
      "source": [
        "**Course 3: Machine Learning Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drLqqHr-y3qR",
        "colab_type": "text"
      },
      "source": [
        "At the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
        "\n",
        "Transformer classes have a .transform() method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class Bucketizer to create discrete bins from a continuous feature or the class PCA to reduce the dimensionality of your dataset using principal component analysis.\n",
        "\n",
        "Estimator classes all implement a .fit() method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a StringIndexerModel for including categorical data saved as strings in your models, or a RandomForestModel that uses the random forest algorithm for classification or regression.\n",
        "\n",
        "#### Spark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals\n",
        "\n",
        "It's important to note that .cast() works on columns, while .withColumn() works on DataFrames. The only argument you need to pass to .cast() is the kind of value you want to create, in string form. For example, to create integers, you'll pass the argument \"integer\" and for decimal numbers you'll use \"double\".\n",
        "\n",
        "dataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\")) \\\\\n",
        "model_data = model_data.withColumn(\"month\", model_data.month.cast('integer'))\n",
        "\n",
        "Convert to an integer \\\\\n",
        "model_data = model_data.withColumn(\"label\", model_data.is_late.cast('integer'))\n",
        "\n",
        "Create a StringIndexer \\\\\n",
        "carr_indexer = StringIndexer(inputCol= 'carrier',outputCol='carrier_index')\n",
        "\n",
        "Create a OneHotEncoder \\\\\n",
        "carr_encoder = OneHotEncoder(inputCol='carrier_index',outputCol='carrier_fact')\n",
        "\n",
        "Make a VectorAssembler \\\\\n",
        "vec_assembler = VectorAssembler(inputCols=['month','air_time','carrier_fact','dest_fact','plane_age'], outputCol='features')\n",
        "\n",
        "Import Pipeline \\\\\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "Make the pipeline \\\\\n",
        "flights_pipe = Pipeline(stages=[dest_indexer,dest_encoder,carr_indexer,carr_encoder,vec_assembler])\n",
        "\n",
        "Fit and transform the data \\\\\n",
        "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
        "\n",
        "Split the data into training and test sets \\\\\n",
        "training, test = piped_data.randomSplit([0.6,0.4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NgxrCHRNswJ",
        "colab_type": "text"
      },
      "source": [
        "**Course 4: Model tuning and selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKaQluTkNz9B",
        "colab_type": "text"
      },
      "source": [
        "Import LogisticRegression \\\\\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "Create a LogisticRegression Estimator \\\\\n",
        "lr = LogisticRegression()\n",
        "\n",
        "Import the evaluation submodule \\\\\n",
        "import pyspark.ml.evaluation as evals\n",
        "\n",
        "Create a BinaryClassificationEvaluator \\\\\n",
        "evaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
        "\n",
        "Import the tuning submodule \\\\\n",
        "import pyspark.ml.tuning as tune\n",
        "\n",
        "Create the parameter grid \\\\\n",
        "grid = tune.ParamGridBuilder()\n",
        "\n",
        "Add the hyperparameter \\\\\n",
        "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
        "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
        "\n",
        "Build the grid \\\\\n",
        "grid = grid.build()\n",
        "\n",
        "Create the CrossValidator \\\\\n",
        "cv = tune.CrossValidator(estimator=lr,\n",
        "               estimatorParamMaps=grid,\n",
        "               evaluator=evaluator\n",
        "               )\n",
        "\n",
        "Use the model to predict the test set \\\\\n",
        "test_results = best_lr.transform(test)\n",
        "\n",
        "Evaluate the predictions \\\\\n",
        "print(evaluator.evaluate(test_results))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDQQ-Le7Kr7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following are from https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "# Install spark-related dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azoG2j7XNc8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ddc07371-58c6-4c1e-f913-142195b1605d"
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5ZK1iuOUy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download datasets directly to your Google Drive \"Colab Datasets\" folder\n",
        "\n",
        "import requests\n",
        "\n",
        "# 2007 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)\n",
        "\n",
        "# 2008 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}