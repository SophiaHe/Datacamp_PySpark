{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data with PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxWB3NYgml1khD9dwy15kk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiaHe/Datacamp_PySpark/blob/master/Big_Data_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJJ1YY4SMWvK",
        "colab_type": "text"
      },
      "source": [
        "**Course 1: Introduction to PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JhIs-ITFKUm",
        "colab_type": "text"
      },
      "source": [
        "See what tables are in your cluster by calling spark.catalog.listTables()\n",
        "\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "Get the first 10 rows of flights: \n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "Show the results:\n",
        "flights10.show()\n",
        "\n",
        "Convert the results to a pandas DataFrame:\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "# Create pd_temp\n",
        "pd_temp = pd.DataFrame(np.random.random(10))\n",
        "\n",
        "# Create spark_temp from pd_temp\n",
        "spark_temp = spark.createDataFrame(pd_temp)\n",
        "\n",
        "# Add spark_temp to the catalog\n",
        "spark_temp.createOrReplaceTempView(\"temp\")\n",
        "\n",
        "# Examine the tables in the catalog again\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# Don't change this file path\n",
        "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
        "\n",
        "# Read in the airports data\n",
        "airports = spark.read.csv(file_path, header = True)\n",
        "\n",
        "# Create the DataFrame flights\n",
        "flights = spark.table(\"flights\")\n",
        "\n",
        "# Show the head\n",
        "flights.show()\n",
        "\n",
        "# Add duration_hrs\n",
        "flights = flights.withColumn('duration_hrs',flights.air_time/60)\n",
        "\n",
        "# Filter flights by passing a string\n",
        "long_flights1 = flights.filter(\"distance > 1000\")\n",
        "\n",
        "# Filter flights by passing a column of boolean values\n",
        "long_flights2 = flights.filter(flights.distance > 1000)\n",
        "\n",
        "# Select the first set of columns\n",
        "selected1 = flights.select(\"tailnum\",\"origin\",\"dest\")\n",
        "\n",
        "# Select the second set of columns\n",
        "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
        "\n",
        "# Define avg_speed\n",
        "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
        "\n",
        "# Create the same table using a SQL expression\n",
        "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
        "\n",
        "# Find the shortest flight from PDX in terms of distance\n",
        "flights.filter(flights.origin == 'PDX').groupBy().min(\"distance\").show()\n",
        "\n",
        "# Average duration of Delta flights\n",
        "flights.filter(flights.carrier == \"DL\").filter(flights.origin == 'SEA').groupBy().avg(\"air_time\").show()\n",
        "\n",
        "# Total hours in the air\n",
        "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDQQ-Le7Kr7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following are from https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "# Install spark-related dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azoG2j7XNc8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ddc07371-58c6-4c1e-f913-142195b1605d"
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5ZK1iuOUy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download datasets directly to your Google Drive \"Colab Datasets\" folder\n",
        "\n",
        "import requests\n",
        "\n",
        "# 2007 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)\n",
        "\n",
        "# 2008 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}