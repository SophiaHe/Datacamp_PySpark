{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data with PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLLTyK30nr98oWG8Q8qa3R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiaHe/Datacamp_PySpark/blob/master/Big_Data_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJJ1YY4SMWvK",
        "colab_type": "text"
      },
      "source": [
        "**Course 1: Introduction to PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JhIs-ITFKUm",
        "colab_type": "text"
      },
      "source": [
        "See what tables are in your cluster by calling spark.catalog.listTables()\n",
        "\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "Get the first 10 rows of flights: \n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "Show the results:\n",
        "flights10.show()\n",
        "\n",
        "Convert the results to a pandas DataFrame:\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "Create pd_temp \\\\\n",
        "pd_temp = pd.DataFrame(np.random.random(10))\n",
        "\n",
        "Create spark_temp from pd_temp \\\\\n",
        "spark_temp = spark.createDataFrame(pd_temp)\n",
        "\n",
        "Add spark_temp to the catalog \\\\\n",
        "spark_temp.createOrReplaceTempView(\"temp\")\n",
        "\n",
        "Examine the tables in the catalog again \\\\\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
        "\n",
        "Read in the airports data \\\\\n",
        "airports = spark.read.csv(file_path, header = True)\n",
        "\n",
        "Create the DataFrame flights \\\\\n",
        "flights = spark.table(\"flights\")\n",
        "\n",
        "Show the head \\\\\n",
        "flights.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH55OT6fyiJO",
        "colab_type": "text"
      },
      "source": [
        "**Course 2: Manipulating Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atfjHIaYytIE",
        "colab_type": "text"
      },
      "source": [
        "Add duration_hrs \\\\\n",
        "flights = flights.withColumn('duration_hrs',flights.air_time/60)\n",
        "\n",
        "Filter flights by passing a string \\\\\n",
        "long_flights1 = flights.filter(\"distance > 1000\")\n",
        "\n",
        "Filter flights by passing a column of boolean values \\\\\n",
        "long_flights2 = flights.filter(flights.distance > 1000)\n",
        "\n",
        "Select the first set of columns \\\\\n",
        "selected1 = flights.select(\"tailnum\",\"origin\",\"dest\")\n",
        "\n",
        "Select the second set of columns \\\\\n",
        "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
        "\n",
        "Define avg_speed \\\\\n",
        "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
        "\n",
        "Create the same table using a SQL expression \\\\\n",
        "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
        "\n",
        "Find the shortest flight from PDX in terms of distance \\\\\n",
        "flights.filter(flights.origin == 'PDX').groupBy().min(\"distance\").show()\n",
        "\n",
        "Average duration of Delta flights \\\\\n",
        "flights.filter(flights.carrier == \"DL\").filter(flights.origin == 'SEA').groupBy().avg(\"air_time\").show()\n",
        "\n",
        "Total hours in the air \\\\\n",
        "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n",
        "\n",
        "Import pyspark.sql.functions as F \\\\\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "Standard deviation of departure delay \\\\\n",
        "by_month_dest.agg(F.stddev('dep_delay')).show()\n",
        "\n",
        "Rename the faa column \\\\\n",
        "airports = airports.withColumnRenamed('faa','dest')\n",
        "\n",
        "Join the DataFrames \\\\\n",
        "flights_with_airports = flights.join(airports,on = 'dest', how = 'leftouter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ERfB7HKy3Q5",
        "colab_type": "text"
      },
      "source": [
        "**Course 3: Machine Learning Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drLqqHr-y3qR",
        "colab_type": "text"
      },
      "source": [
        "At the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
        "\n",
        "Transformer classes have a .transform() method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class Bucketizer to create discrete bins from a continuous feature or the class PCA to reduce the dimensionality of your dataset using principal component analysis.\n",
        "\n",
        "Estimator classes all implement a .fit() method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a StringIndexerModel for including categorical data saved as strings in your models, or a RandomForestModel that uses the random forest algorithm for classification or regression.\n",
        "\n",
        "#### Spark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals\n",
        "\n",
        "It's important to note that .cast() works on columns, while .withColumn() works on DataFrames. The only argument you need to pass to .cast() is the kind of value you want to create, in string form. For example, to create integers, you'll pass the argument \"integer\" and for decimal numbers you'll use \"double\".\n",
        "\n",
        "dataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\")) \\\\\n",
        "model_data = model_data.withColumn(\"month\", model_data.month.cast('integer'))\n",
        "\n",
        "Convert to an integer \\\\\n",
        "model_data = model_data.withColumn(\"label\", model_data.is_late.cast('integer'))\n",
        "\n",
        "Create a StringIndexer \\\\\n",
        "carr_indexer = StringIndexer(inputCol= 'carrier',outputCol='carrier_index')\n",
        "\n",
        "Create a OneHotEncoder \\\\\n",
        "carr_encoder = OneHotEncoder(inputCol='carrier_index',outputCol='carrier_fact')\n",
        "\n",
        "Make a VectorAssembler \\\\\n",
        "vec_assembler = VectorAssembler(inputCols=['month','air_time','carrier_fact','dest_fact','plane_age'], outputCol='features')\n",
        "\n",
        "Import Pipeline \\\\\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "Make the pipeline \\\\\n",
        "flights_pipe = Pipeline(stages=[dest_indexer,dest_encoder,carr_indexer,carr_encoder,vec_assembler])\n",
        "\n",
        "Fit and transform the data \\\\\n",
        "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
        "\n",
        "Split the data into training and test sets \\\\\n",
        "training, test = piped_data.randomSplit([0.6,0.4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NgxrCHRNswJ",
        "colab_type": "text"
      },
      "source": [
        "**Course 4: Model tuning and selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKaQluTkNz9B",
        "colab_type": "text"
      },
      "source": [
        "Import LogisticRegression \\\\\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "Create a LogisticRegression Estimator \\\\\n",
        "lr = LogisticRegression()\n",
        "\n",
        "Import the evaluation submodule \\\\\n",
        "import pyspark.ml.evaluation as evals\n",
        "\n",
        "Create a BinaryClassificationEvaluator \\\\\n",
        "evaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
        "\n",
        "Import the tuning submodule \\\\\n",
        "import pyspark.ml.tuning as tune\n",
        "\n",
        "Create the parameter grid \\\\\n",
        "grid = tune.ParamGridBuilder()\n",
        "\n",
        "Add the hyperparameter \\\\\n",
        "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
        "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
        "\n",
        "Build the grid \\\\\n",
        "grid = grid.build()\n",
        "\n",
        "Create the CrossValidator \\\\\n",
        "cv = tune.CrossValidator(estimator=lr,\n",
        "               estimatorParamMaps=grid,\n",
        "               evaluator=evaluator\n",
        "               )\n",
        "\n",
        "Use the model to predict the test set \\\\\n",
        "test_results = best_lr.transform(test)\n",
        "\n",
        "Evaluate the predictions \\\\\n",
        "print(evaluator.evaluate(test_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1htp678qTXO0",
        "colab_type": "text"
      },
      "source": [
        "###**Big data foundation with PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfhM27ueTJTd",
        "colab_type": "text"
      },
      "source": [
        "**Course 1: Introduction to big data analysis in Spark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guiOE0fFThzC",
        "colab_type": "text"
      },
      "source": [
        "Print the version of SparkContext \\\\\n",
        "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
        "\n",
        "Print the Python version of SparkContext \\\\\n",
        "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
        "\n",
        "Print the master of SparkContext \\\\\n",
        "print(\"The master of Spark Context in the PySpark shell is\", sc.master)\n",
        "\n",
        "Create a python list of numbers from 1 to 100 \\\\\n",
        "numb = range(1, 100)\n",
        "\n",
        "Load the list into PySpark  \\\\\n",
        "spark_data = sc.parallelize(numb)\n",
        "\n",
        "Load a local file into PySpark shell \\\\\n",
        "lines = sc.textFile(file_path)\n",
        "\n",
        "**Use of lambda() with map():** \\\\\n",
        "The map() function in Python returns a list of the results after applying the given function to each item of a given iterable (list, tuple etc.). The general syntax of map() function is map(fun, iter). We can also use lambda functions with map(). The general syntax of map() function with lambda() is map(lambda <agument>:<expression>, iter)\n",
        "\n",
        "Square all numbers in my_list \\\\\n",
        "squared_list_lambda = list(map(lambda x: x ** 2, my_list))\n",
        "\n",
        "Print the result of the map function \\\\\n",
        "print(\"The squared numbers are\", squared_list_lambda)\n",
        "\n",
        "**Use of lambda() with filter():** \\\\\n",
        "Another function that is used extensively in Python is the filter() function. The filter() function in Python takes in a function and a list as arguments. The general syntax of the filter() function is filter(function, list_of_input). Similar to the map(), filter() can be used with lambda() function. The general syntax of the filter() function with lambda() is filter(lambda <argument>:<expression>, list)\n",
        "\n",
        "Filter numbers divisible by 10 \\\\\n",
        "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0sVpyzuY-sx",
        "colab_type": "text"
      },
      "source": [
        "**Course 2: Programming in PySpark RDD’s**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz8xlWOwZBTM",
        "colab_type": "text"
      },
      "source": [
        "**Create an RDD from a list of words \\\\\n",
        "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])**\n",
        "\n",
        "Print out the type of the created object \\\\\n",
        "print(\"The type of RDD is\", type(RDD))\n",
        "\n",
        "**Create a fileRDD from file_path \\\\\n",
        "fileRDD = sc.textFile(file_path)**\n",
        "\n",
        "Create a fileRDD_part from file_path with 5 partitions \\\\\n",
        "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
        "\n",
        "Check the number of partitions in fileRDD_part \\\\\n",
        "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())\n",
        "\n",
        "**Create map() transformation to cube numbers \\\\\n",
        "cubedRDD = numbRDD.map(lambda x: x ** 3)**\n",
        "\n",
        "Collect the results \\\\\n",
        "numbers_all = cubedRDD.collect()\n",
        "\n",
        "Print the numbers from numbers_all \\\\\n",
        "for numb in numbers_all:\n",
        "\tprint(numb)\n",
        "\n",
        "**Filter the fileRDD to select lines with Spark keyword \\\\\n",
        "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)**\n",
        "\n",
        "**How many lines are there in fileRDD? \\\\\n",
        "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())**\n",
        "\n",
        "Print the first four lines of fileRDD \\\\\n",
        "for line in fileRDD_filter.take(4): \n",
        "  print(line)\n",
        "\n",
        "Create PairRDD Rdd with key value pairs \\\\\n",
        "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
        "\n",
        "**Apply reduceByKey() operation on Rdd \\\\\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)**\n",
        "\n",
        "Iterate over the result and print the output \\\\\n",
        "for num in Rdd_Reduced.collect(): \n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
        "\n",
        ":Key 4 has 5 Counts\n",
        "Key 1 has 2 Counts\n",
        "Key 3 has 10 Counts\n",
        "\n",
        "**Sort the reduced RDD with the key by descending order \\\\\n",
        "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)**\n",
        "\n",
        "Iterate over the result and print the output \\\\\n",
        "for num in Rdd_Reduced_Sort.collect():\n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
        "\n",
        ": Key 4 has 5 Counts\n",
        "Key 3 has 10 Counts\n",
        "Key 1 has 2 Counts\n",
        "\n",
        "**Transform the rdd with countByKey() \\\\\n",
        "total = Rdd.countByKey()**\n",
        "\n",
        "What is the type of total? \\\\\n",
        "print(\"The type of total is\", type(total))\n",
        "\n",
        "**Iterate over the total and print the output \\\\\n",
        "for k, v in total.items(): \n",
        "  print(\"key\", k, \"has\", v, \"counts\")**\n",
        "\n",
        ":The type of total is <class 'collections.defaultdict'>\n",
        "key 1 has 2 counts\n",
        "key 1 has 2 counts\n",
        "key 1 has 2 counts\n",
        "\n",
        "Create a baseRDD from the file path \\\\\n",
        "baseRDD = sc.textFile(file_path)\n",
        "\n",
        "Split the lines of baseRDD into words \\\\\n",
        "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
        "\n",
        "Count the total number of words \\\\\n",
        "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
        "\n",
        "Convert the words in lower case and remove stop words from stop_words \\\\\n",
        "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
        "\n",
        "Create a tuple of the word and 1 \\\\\n",
        "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
        "\n",
        "Count of the number of occurences of each word \\\\\n",
        "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "Display the first 10 words and their frequencies \\\\\n",
        "for word in resultRDD.take(10):\n",
        "\tprint(word)\n",
        "\n",
        "Swap the keys and values \\\\\n",
        "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "Sort the keys in descending order \\\\\n",
        "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
        "\n",
        "Show the top 10 most frequent words and their frequencies \\\\\n",
        "for word in resultRDD_swap_sort.take(10):\n",
        "\tprint(\"{} has {} counts\". format(word[1], word[0]))\n",
        "\n",
        ":   thou has 4247 counts\n",
        "    thy has 3630 counts\n",
        "    shall has 3018 counts\n",
        "    good has 2046 counts\n",
        "    would has 1974 counts\n",
        "    Enter has 1926 counts\n",
        "    thee has 1780 counts\n",
        "    I'll has 1737 counts\n",
        "    hath has 1614 counts\n",
        "    like has 1452 counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpV8bq7iNsBM",
        "colab_type": "text"
      },
      "source": [
        "**Course 3: PySpark SQL & DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJINJXdmNxV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Create a list of tuples \\\\\n",
        "sample_list = [('Mona',20), ('Jennifer',34),('John',20), ('Jim',26)]\n",
        "\n",
        "Create a RDD from the list \\\\\n",
        "rdd = sc.parallelize(sample_list)\n",
        "\n",
        "Create a PySpark DataFrame \\\\\n",
        "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
        "\n",
        "Check the type of names_df \\\\\n",
        "print(\"The type of names_df is\", type(names_df))\n",
        "\n",
        "Create an DataFrame from file_path \\\\\n",
        "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "Print the first 10 observations \\\\\n",
        "people_df.show(10)\n",
        "\n",
        "Count the number of rows \\\\\n",
        "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
        "\n",
        "Count the number of columns and their names \\\\\n",
        "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))\n",
        "\n",
        "Select name, sex and date of birth columns\\\\\n",
        "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
        "\n",
        "Print the first 10 observations from people_df_sub\\\\\n",
        "people_df_sub.show(10)\n",
        "\n",
        "Remove duplicate entries from people_df_sub\\\\\n",
        "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
        "\n",
        "Filter people_df to select females \\\\\n",
        "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
        "\n",
        "Create a temporary table \"people\" \\\\\n",
        "people_df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "Construct a query to select the names of the people from the temporary table \"people\" \\\\\n",
        "query = '''SELECT name FROM people'''\n",
        "\n",
        "Assign the result of Spark's query to people_df_names \\\\\n",
        "people_df_names = spark.sql(query)\n",
        "\n",
        "Filter the people table to select female sex \\\\\n",
        "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
        "\n",
        "Check the schema of columns \\\\\n",
        "fifa_df.printSchema()\n",
        "\n",
        "Summary of numeric cols \\\\\n",
        "spark_dataFrame.describe()\n",
        "\n",
        "**Vis using toPandas, HandySpark**\n",
        "Check the column names of names_df\\\\\n",
        "print(\"The column names of names_df are\", names_df.columns)\n",
        "\n",
        "Convert to Pandas DataFrame  \\\\\n",
        "df_pandas = names_df.toPandas()\n",
        "\n",
        "Create a horizontal bar plot \\\\\n",
        "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la4JxuzqLNml",
        "colab_type": "text"
      },
      "source": [
        "**Course 4: PySpark MLlib Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHUM3jOWLh19",
        "colab_type": "text"
      },
      "source": [
        "Import the library for ALS \\\\\n",
        "from pyspark.mllib.recommendation import ALS\n",
        "\n",
        "Import the library for Logistic Regression \\\\\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "\n",
        "Import the library for Kmeans \\\\\n",
        "from pyspark.mllib.clustering import KMeans\n",
        "\n",
        "**1. Collaborative Filtering** \\\\\n",
        "Finding users that shares common interests; Recommendation system; \\\\\n",
        "\n",
        "Load the data into RDD \\\\\n",
        "data = sc.textFile(file_path)\n",
        "\n",
        "Split the RDD \\\\\n",
        "ratings = data.map(lambda l: l.split(','))\n",
        "\n",
        "Transform the ratings RDD \\\\\n",
        "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
        "\n",
        "Split the data into training and test \\\\\n",
        "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])\n",
        "\n",
        "Create the ALS model on the training data \\\\\n",
        "model = ALS.train(training_data, rank=10, iterations=10)\n",
        "\n",
        "Drop the ratings column \\\\\n",
        "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
        "\n",
        "Predict the model  \\\\\n",
        "predictions = model.predictAll(testdata_no_rating)\n",
        "\n",
        "Print the first rows of the RDD \\\\\n",
        "predictions.take(2)\n",
        "\n",
        "Prepare ratings data \\\\\n",
        "rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
        "\n",
        "Prepare predictions data \\\\\n",
        "preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
        "\n",
        "Join the ratings data with predictions data \\\\\n",
        "rates_and_preds = rates.join(preds)\n",
        "\n",
        "Calculate and print MSE \\\\\n",
        "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))\n",
        "\n",
        "\n",
        "**2. Classification**\n",
        "Load the datasets into RDDs \\\\\n",
        "spam_rdd = sc.textFile(file_path_spam)\n",
        "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
        "\n",
        "Split the email messages into words \\\\\n",
        "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
        "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
        "\n",
        "Print the first element in the split RDD \\\\\n",
        "print(\"The first element in spam_words is\", spam_words.first())\n",
        "print(\"The first element in non_spam_words is\", non_spam_words.first())\n",
        "\n",
        ":    The first element in spam_words is You\n",
        "    The first element in non_spam_words is Rofl.\n",
        "\n",
        "Create a HashingTf instance with 200 features \\\\\n",
        "tf = HashingTF(numFeatures=200)\n",
        "\n",
        "Map each word to one feature \\\\\n",
        "spam_features = tf.transform(spam_words)\n",
        "non_spam_features = tf.transform(non_spam_words)\n",
        "\n",
        "Label the features: 1 for spam, 0 for non-spam \\\\\n",
        "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
        "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
        "\n",
        "Combine the two datasets \\\\\n",
        "samples = spam_samples.join(non_spam_samples)\n",
        "\n",
        "Split the data into training and testing \\\\\n",
        "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
        "\n",
        "Train the model \\\\\n",
        "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
        "\n",
        "Create a prediction label from the test data \\\\\n",
        "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
        "\n",
        "Combine original labels with the predicted labels \\\\\n",
        "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
        "\n",
        "Check the accuracy of the model on the test data \\\\\n",
        "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
        "print(\"Model accuracy : {:.2f}\".format(accuracy))\n",
        "\n",
        "**3. Clustering**\n",
        "\n",
        "Load the dataset into a RDD \\\\\n",
        "clusterRDD = sc.textFile(file_path)\n",
        "\n",
        "Split the RDD based on tab \\\\\n",
        "rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))\n",
        "\n",
        "Transform the split RDD by creating a list of integers \\\\\n",
        "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
        "\n",
        "Count the number of rows in RDD \\\\\n",
        "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))\n",
        "\n",
        "Train the model with clusters from 13 to 16 and compute WSSSE \\\\\n",
        "for clst in range(13, 17):\n",
        "    model = KMeans.train(rdd_split_int, clst, seed=1)\n",
        "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))\n",
        "\n",
        ":The cluster 13 has Within Set Sum of Squared Error 249164132.49410182\n",
        "The cluster 14 has Within Set Sum of Squared Error 209371154.24941802\n",
        "The cluster 15 has Within Set Sum of Squared Error 169394691.52639425\n",
        "\n",
        "Train the model again with the best k \\\\\n",
        "model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
        "\n",
        "Get cluster centers \\\\\n",
        "cluster_centers = model.clusterCenters\n",
        "\n",
        "Convert rdd_split_int RDD into Spark DataFrame \\\\\n",
        "rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\n",
        "\n",
        "Convert Spark DataFrame into Pandas DataFrame \\\\\n",
        "rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n",
        "\n",
        "Convert \"cluster_centers\" that you generated earlier into Pandas DataFrame \\\\\n",
        "cluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\n",
        "\n",
        "Create an overlaid scatter plot \\\\\n",
        "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
        "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7J0WSIze1R",
        "colab_type": "text"
      },
      "source": [
        "## **Cleaning Data with PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlHtBsU53qxH",
        "colab_type": "text"
      },
      "source": [
        "**Course 1: DataFrame details**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5DXOuGH30zM",
        "colab_type": "text"
      },
      "source": [
        "Import the pyspark.sql.types library \\\\\n",
        "`from pyspark.sql.types import *`\n",
        "\n",
        "**Define a new schema using the StructType method** \\\\\n",
        "\"Define a StructField for each field\" \\\\\n",
        "`people_schema = StructType([ \\\\\n",
        "  StructField('name', StringType(), False), \\\\\n",
        "  StructField('age', IntegerType(), False), \\\\\n",
        "  StructField('city', StringType(), False) \\\\\n",
        "])`\n",
        "\n",
        "Load the CSV file \\\\\n",
        "`aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')`\n",
        "\n",
        "Add the airport column using the F.lower() method \\\\\n",
        "`aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))`\n",
        "\n",
        "Drop the Destination Airport column \\\\\n",
        "`aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])`\n",
        "\n",
        "Show the DataFrame \\\\\n",
        "`aa_dfw_df.show()`\n",
        "\n",
        "The **Parquet format** is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
        "\n",
        "Save the df3 DataFrame in Parquet format \\\\\n",
        "`df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')`\n",
        "\n",
        "Read the Parquet file into a new DataFrame and run a count \\\\\n",
        "`print(spark.read.parquet('AA_DFW_ALL.parquet').count())`\n",
        "\n",
        "**Run SQL query on Parquet data**\n",
        "Read the Parquet file into flights_df \\\\\n",
        "`flights_df = spark.read.parquet('AA_DFW_ALL.parquet')`\n",
        "\n",
        "Register the temp table \\\\\n",
        "`flights_df.createOrReplaceTempView('flights')`\n",
        "\n",
        "Run a SQL query of the average flight duration \\\\\n",
        "`avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]`\n",
        "\n",
        "`print('The average flight time is: %d' % avg_duration)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ-gwiM6Od-Z",
        "colab_type": "text"
      },
      "source": [
        "**Course 2: Manipulating DataFrames in the real world**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8eE3-S5elU4"
      },
      "source": [
        "**Show the distinct VOTER_NAME entries** \\\\\n",
        "`voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)`\n",
        "\n",
        "Filter voter_df where the VOTER_NAME is 1-20 characters in length \\\\\n",
        "`voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')`\n",
        "\n",
        "**Filter out voter_df where the VOTER_NAME contains an underscore** \\\\\n",
        "`voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))`\n",
        "\n",
        "Show the distinct VOTER_NAME entries again \\\\\n",
        "`voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)`\n",
        "\n",
        "**Add a new column called splits separated on whitespace** \\\\\n",
        "`voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))`\n",
        "\n",
        "Create a new column called first_name based on the first item in splits \\\\\n",
        "`voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))`\n",
        "\n",
        "**Get the last entry of the splits list and create a column called last_name** \\\\\n",
        "`voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))`\n",
        "\n",
        "Drop the splits column \\\\\n",
        "`voter_df = voter_df.drop('splits')`\n",
        "\n",
        "Add a column to voter_df for any voter with the title 'Councilmember' \\\\\n",
        "`voter_df = voter_df.withColumn('random_val',\n",
        "                               when(voter_df.TITLE == 'Councilmember', F.rand()))`\n",
        "\n",
        "**Add a column to voter_df for a voter based on their position** \\\\\n",
        "`voter_df = voter_df.withColumn('random_val',\n",
        "                               when(voter_df.TITLE == 'Councilmember', F.rand())\n",
        "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
        "                               .otherwise(0))`\n",
        "\n",
        "Use the .filter() clause with random_val \\\\\n",
        "`voter_df.filter(voter_df.random_val == 0).show()`\n",
        "\n",
        "Return a space separated string of names \\\\\n",
        "`def getFirstAndMiddle(names):`\n",
        "  `return ' '.join(names[:-1])`\n",
        "\n",
        "**Define the method as a UDF** \\\\\n",
        "`udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())`\n",
        "\n",
        "Create a new column using your UDF \\\\\n",
        "`voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))`\n",
        "\n",
        "Drop the unnecessary columns then show the DataFrame \\\\\n",
        "`voter_df = voter_df.drop('first_name')`\n",
        "`voter_df = voter_df.drop('splits')`\n",
        "\n",
        "Select all the unique council voters \\\\\n",
        "`voter_df = df.select(df[\"VOTER NAME\"]).distinct()`\n",
        "\n",
        "Count the rows in voter_df \\\\\n",
        "`print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())`\n",
        "\n",
        "Add a ROW_ID \\\\\n",
        "`voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())`\n",
        "\n",
        "Show the rows with 10 highest IDs in the set \\\\\n",
        "`voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)`\n",
        "\n",
        "**Print the number of partitions in each DataFrame** \\\\\n",
        "`print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())`\n",
        "`print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())`\n",
        "\n",
        "**Add a ROW_ID field to each DataFrame** \\\\\n",
        "`voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())`\n",
        "`voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())`\n",
        "\n",
        "Show the top 10 IDs in each DataFrame  \\\\\n",
        "`voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)`\n",
        "`voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)`\n",
        "\n",
        "Determine the highest ROW_ID and save it in previous_max_ID \\\\\n",
        "`previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]`\n",
        "\n",
        "Add a ROW_ID column to voter_df_april starting at the desired value \\\\\n",
        "`voter_df_april = voter_df_april.withColumn('ROW_ID', previous_max_ID + F.monotonically_increasing_id())`\n",
        "\n",
        "Show the ROW_ID from both DataFrames and compare \\\\\n",
        "`voter_df_april.select('ROW_ID').show()`\n",
        "`voter_df_march.select('ROW_ID').show()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p64P4O5wym9k",
        "colab_type": "text"
      },
      "source": [
        "**Course 3:Improving Performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz50QmYGyp3S",
        "colab_type": "text"
      },
      "source": [
        "Add caching to the unique rows in departures_df \\\\\n",
        "`departures_df = departures_df.distinct().cache()`\n",
        "\n",
        "Remove departures_df from the cache \\\\\n",
        "`departures_df.unpersist()`\n",
        "\n",
        "Check the cache status again \\\\\n",
        "`print(\"Is departures_df cached?: %s\" % departures_df.is_cached)`\n",
        "\n",
        "Import the full and split files into DataFrames \\\\\n",
        "`full_df = spark.read.csv('departures_full.txt.gz')`\n",
        "\n",
        "`split_df = spark.read.csv('departures_*.txt.gz')`\n",
        "\n",
        "Name of the Spark application instance \\\\\n",
        "`app_name = spark.conf.get('spark.app.name')`\n",
        "\n",
        "Driver TCP port \\\\\n",
        "`driver_tcp_port = spark.conf.get('spark.driver.port')`\n",
        "\n",
        "Number of join partitions \\\\\n",
        "`num_partitions = spark.conf.get('spark.sql.shuffle.partitions')`\n",
        "\n",
        "Show the results \\\\\n",
        "`print(\"Name: %s\" % app_name)`\n",
        "\n",
        "`print(\"Driver TCP port: %s\" % driver_tcp_port)`\n",
        "\n",
        "`print(\"Number of partitions: %s\" % num_partitions)`\n",
        "\n",
        "Configure Spark to use 500 partitions \\\\\n",
        "`spark.conf.set('spark.sql.shuffle.partitions', 500)`\n",
        "\n",
        "Import the broadcast method from pyspark.sql.functions \\\\\n",
        "`from pyspark.sql.functions import broadcast`\n",
        "\n",
        "Join the flights_df and airports_df DataFrames using broadcasting \\\\\n",
        "`broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
        "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )`\n",
        "\n",
        "Show the query plan and compare against the original \\\\\n",
        "`broadcast_df.explain()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_THHJat8x99",
        "colab_type": "text"
      },
      "source": [
        "**Course 4: Complex processing and data pipelines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKp5_v3p80D1",
        "colab_type": "text"
      },
      "source": [
        "Import the data to a DataFrame \\\\\n",
        "`departures_df = spark.read.csv('2015-departures.csv.gz', header=True)`\n",
        "\n",
        "Remove any duration of 0 \\\\\n",
        "`departures_df = departures_df.filter(departures_df[3] >0)`\n",
        "\n",
        "Add an ID column \\\\\n",
        "`departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())`\n",
        "\n",
        "Write the file out to JSON format \\\\\n",
        "`departures_df.write.json('output.json')`\n",
        "\n",
        "Import the file to a DataFrame and perform a row count \\\\ \n",
        "`annotations_df = spark.read.csv('annotations.csv.gz', sep='|')`\n",
        "\n",
        "`full_count = annotations_df.count()`\n",
        "\n",
        "Count the number of rows beginning with '#' \\\\\n",
        "`comment_count = annotations_df.where(col('_c0').startswith('#')).count()`\n",
        "\n",
        "Import the file to a new DataFrame, without commented rows \\\\\n",
        "`no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')`\n",
        "\n",
        "Count the new DataFrame and verify the difference is as expected \\\\\n",
        "`no_comments_count = no_comments_df.count()`\n",
        "\n",
        "`print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))`\n",
        "\n",
        "Import the file to a DataFrame and perform a row count \\\\\n",
        "`annotations_df = spark.read.csv('annotations.csv.gz', sep='|')`\n",
        "\n",
        "`full_count = annotations_df.count()`\n",
        "\n",
        "Count the number of rows beginning with '#' \\\\\n",
        "`comment_count = annotations_df.where(col('_c0').startswith('#')).count()`\n",
        "\n",
        "**Import the file to a new DataFrame, without commented rows** \\\\\n",
        "`no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')`\n",
        "\n",
        "Count the new DataFrame and verify the difference is as expected \\\\\n",
        "`no_comments_count = no_comments_df.count()`\n",
        "\n",
        "`print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))`\n",
        "\n",
        "Split _c0 on the tab character and store the list in a variable \\\\\n",
        "`tmp_fields = F.split(annotations_df['_c0'], '\\t')`\n",
        "\n",
        "Create the colcount column on the DataFrame \\\\\n",
        "`annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))`\n",
        "\n",
        "Remove any rows containing fewer than 5 fields \\\\\n",
        "`annotations_df_filtered = annotations_df.filter(~ (annotations_df.colcount < 5))`\n",
        "\n",
        "Count the number of rows \\\\\n",
        "`final_count = annotations_df_filtered.count()`\n",
        "\n",
        "`print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDQQ-Le7Kr7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following are from https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "# Install spark-related dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azoG2j7XNc8z",
        "colab_type": "code",
        "outputId": "ddc07371-58c6-4c1e-f913-142195b1605d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5ZK1iuOUy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download datasets directly to your Google Drive \"Colab Datasets\" folder\n",
        "\n",
        "import requests\n",
        "\n",
        "# 2007 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)\n",
        "\n",
        "# 2008 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}